# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qQr6M7nWnCrg9jeyv1dIY_WaSkqQ7Rj9
"""

import numpy as np
import pandas as pd
import transformers

import torch
import torch.nn as nn
from torch.utils.data import Dataset,DataLoader

df=pd.read_csv('/content/drive/MyDrive/AI Hackathon/images.csv')

df

df['image_url'][0]

df2=pd.read_excel('/content/drive/MyDrive/AI Hackathon/product_data.xlsx')

df2

import pandas as pd

# df: contains multiple image_urls per id
# df2: contains one row per id with title, description, product_type

# Step 1: Trim df2 to required columns
df2_trimmed = df2[['id', 'title', 'description', 'product_type']]

# Step 2: Merge all images from df with metadata from df2
final_df = pd.merge(df, df2_trimmed, on='id', how='inner')

# Step 3: Optional: rearrange columns
final_df = final_df[['id', 'image_url', 'title', 'description', 'product_type']]

# Step 4: Shuffle the dataframe
final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Preview
print(final_df.head())

final_df

"""Exploring more yolo models

"""

from transformers import AutoImageProcessor, AutoModelForObjectDetection
from PIL import Image, ImageDraw, ImageFont
import torch
import matplotlib.pyplot as plt

# Load the image processor and model
processor = AutoImageProcessor.from_pretrained("valentinafeve/yolos-fashionpedia")
model = AutoModelForObjectDetection.from_pretrained("valentinafeve/yolos-fashionpedia")
model.eval()

# Load your image
image_path = "/content/drive/MyDrive/AI Hackathon/videos/2025-05-27_13-46-16_UTC.jpg"
image = Image.open(image_path).convert("RGB")

# Preprocess the image
inputs = processor(images=image, return_tensors="pt")

# Inference
with torch.no_grad():
    outputs = model(**inputs)

# Post-process outputs (get boxes, scores, labels)
target_sizes = torch.tensor([image.size[::-1]])  # (height, width)
results = processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

# Category names from model card
CATS = [
    'shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest',
    'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat',
    'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer',
    'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood',
    'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper',
    'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel'
]

# Draw boxes
draw = ImageDraw.Draw(image)
for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    label_name = CATS[label]
    draw.rectangle(box, outline="red", width=3)
    draw.text((box[0], box[1] - 10), f"{label_name} ({score:.2f})", fill="red")

# Display image with boxes
plt.figure(figsize=(12, 8))
plt.imshow(image)
plt.axis("off")
plt.title("YOLOS-Fashionpedia Detection")
plt.show()

from transformers import AutoImageProcessor, AutoModelForObjectDetection
from PIL import Image, ImageDraw, ImageFont
import torch
import matplotlib.pyplot as plt
import requests
import io

# ------------------ Setup ------------------
processor = AutoImageProcessor.from_pretrained("valentinafeve/yolos-fashionpedia")
model = AutoModelForObjectDetection.from_pretrained("valentinafeve/yolos-fashionpedia")
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Load image (from path or URL)
image_path = "/content/drive/MyDrive/AI Hackathon/videos/2025-05-27_13-46-16_UTC.jpg"
image = Image.open(image_path).convert("RGB")
draw = ImageDraw.Draw(image)

# ------------------ Category Map ------------------
CATS = [
    'shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest',
    'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat',
    'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer',
    'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood',
    'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper',
    'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel'
]

custom_label_map = {
    # Tops
    "shirt, blouse": "top",
    "top, t-shirt, sweatshirt": "top",
    "sweater": "top",
    "cardigan": "top",
    "vest": "top",

    # Bottoms
    "pants": "bottom",
    "shorts": "bottom",
    "skirt": "bottom",

    # Dresses
    "dress": "dress",
    "jumpsuit": "dress",

    # Jackets
    "jacket": "jacket",
    "coat": "jacket",

    # Accessories
    "shoe": "shoes",
    "bag, wallet": "bag",
    "watch": "accessory",
    "glasses": "accessory",
    "hat": "accessory",
    "headband, head covering, hair accessory": "accessory"
}

# ------------------ Inference ------------------
inputs = processor(images=image, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)

target_sizes = torch.tensor([image.size[::-1]]).to(device)
results = processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

# ------------------ Drawing Filtered Results ------------------
font = ImageFont.load_default()
for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    label_name = CATS[label.item()]
    if label_name in custom_label_map:
        mapped_label = custom_label_map[label_name]
        score = round(score.item(), 2)
        box = [int(x) for x in box.tolist()]
        draw.rectangle(box, outline="red", width=3)
        draw.text((box[0], box[1]-10), f"{mapped_label} ({score})", fill="white", font=font)

# ------------------ Show Image ------------------
plt.figure(figsize=(12, 8))
plt.imshow(image)
plt.axis("off")
plt.title("Filtered Fashion Detection")
plt.show()

from transformers import AutoImageProcessor, AutoModelForObjectDetection
from PIL import Image, ImageDraw, ImageFont
import torch
import matplotlib.pyplot as plt

# ------------------ Setup ------------------
processor = AutoImageProcessor.from_pretrained("valentinafeve/yolos-fashionpedia")
model = AutoModelForObjectDetection.from_pretrained("valentinafeve/yolos-fashionpedia")
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Load image
image_path = "/content/drive/MyDrive/AI Hackathon/videos/2025-05-27_13-46-16_UTC.jpg"
image = Image.open(image_path).convert("RGB")
draw = ImageDraw.Draw(image)

# ------------------ Category Map ------------------
CATS = [
    'shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest',
    'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat',
    'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer',
    'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood',
    'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper',
    'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel'
]

custom_label_map = {
    "shirt, blouse": "top", "top, t-shirt, sweatshirt": "top", "sweater": "top",
    "cardigan": "top", "vest": "top", "pants": "bottom", "shorts": "bottom",
    "skirt": "bottom", "dress": "dress", "jumpsuit": "dress", "jacket": "jacket",
    "coat": "jacket", "shoe": "shoes", "bag, wallet": "bag", "watch": "accessory",
    "glasses": "accessory", "hat": "accessory", "headband, head covering, hair accessory": "accessory"
}

# ------------------ Inference ------------------
inputs = processor(images=image, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)

target_sizes = torch.tensor([image.size[::-1]]).to(device)
results = processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

# ------------------ Font Setup ------------------
try:
    # You can change the font path below if you want a specific font
    font = ImageFont.truetype("DejaVuSans-Bold.ttf", size=28)
 # Larger font size
except:
    font = ImageFont.load_default()

# ------------------ Drawing Filtered Results ------------------
for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    label_name = CATS[label.item()]
    if label_name in custom_label_map:
        mapped_label = custom_label_map[label_name]
        score = round(score.item(), 2)
        box = [int(x) for x in box.tolist()]
        draw.rectangle(box, outline="red", width=3)
        draw.text((box[0], max(0, box[1]-25)), f"{mapped_label} ({score})", fill="black", font=font)

# ------------------ Show Image ------------------
plt.figure(figsize=(12, 8))
plt.imshow(image)
plt.axis("off")
plt.title("Filtered Fashion Detection")
plt.show()

"""Final Experiment

Loading the valentinafeve model from huggingface which is a fine-tunned object detection model for fashion. This  model supports the following categories:

CATS = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']
"""

import os
import requests
import torch
import pandas as pd
from PIL import Image
from io import BytesIO
from tqdm import tqdm
from transformers import AutoImageProcessor, AutoModelForObjectDetection

processor = AutoImageProcessor.from_pretrained("valentinafeve/yolos-fashionpedia")
model = AutoModelForObjectDetection.from_pretrained("valentinafeve/yolos-fashionpedia").eval().cuda() if torch.cuda.is_available() else AutoModelForObjectDetection.from_pretrained("valentinafeve/yolos-fashionpedia").eval()

"""This script downloads images from URLs, uses a YOLOS Fashionpedia model to detect clothing/accessories, crops detected items, generates natural-language prompts for each crop, saves the crops locally, and exports metadata with prompts to a CSV for further use."""

import os
import requests
import torch
import pandas as pd
import random
from PIL import Image
from io import BytesIO
from tqdm import tqdm
from transformers import AutoImageProcessor, AutoModelForObjectDetection

df = final_df.copy()
save_dir = "catalog_crops"
os.makedirs(save_dir, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

det_ckpt = "valentinafeve/yolos-fashionpedia"
processor = AutoImageProcessor.from_pretrained(det_ckpt)
model = AutoModelForObjectDetection.from_pretrained(det_ckpt).to(device).eval()

CATS = [
    'shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest',
    'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat',
    'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer',
    'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood',
    'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper',
    'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel'
]

custom_label_map = {
    "shirt, blouse": "top", "top, t-shirt, sweatshirt": "top", "sweater": "top",
    "cardigan": "top", "vest": "top", "pants": "bottom", "shorts": "bottom",
    "skirt": "bottom", "dress": "dress", "jumpsuit": "dress", "jacket": "jacket",
    "coat": "jacket", "shoe": "shoes", "bag, wallet": "bag", "watch": "accessory",
    "glasses": "accessory", "hat": "accessory", "headband, head covering, hair accessory": "accessory"
}

prompt_templates = [
    "A photo of a {}.",
    "An image showing a {}.",
    "A close-up of a {}.",
    "Someone wearing a {}.",
    "Fashion product: {}."
]

results = []

for idx, row in tqdm(df.iterrows(), total=len(df)):
    try:
        image_url = row["image_url"]
        image_id = row["id"]

        response = requests.get(image_url, timeout=10)
        image = Image.open(BytesIO(response.content)).convert("RGB")

        inputs = processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model(**inputs)

        target_sizes = torch.tensor([image.size[::-1]]).to(device)
        processed = processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

        for i, (score, label, box) in enumerate(zip(processed["scores"], processed["labels"], processed["boxes"])):
            label_name = CATS[label.item()]
            if label_name not in custom_label_map:
                continue

            class_label = custom_label_map[label_name]
            x1, y1, x2, y2 = map(int, box.tolist())
            cropped = image.crop((x1, y1, x2, y2))

            filename = f"{image_id}_{class_label}_{i}.jpg"
            path = os.path.join(save_dir, filename)
            cropped.save(path)

            prompt = random.choice(prompt_templates).format(class_label)

            results.append({
                "id": image_id,
                "image_path": path,
                "label": class_label,
                "text": prompt,
                "image_url": image_url,
                "bbox": [x1, y1, x2, y2],
                "score": float(score)
            })

    except Exception as e:
        print(f"Error on row {idx} ({row['image_url']}): {e}")
        continue

crop_df = pd.DataFrame(results)
crop_df.to_csv("clip_crops_dataset_with_prompts.csv", index=False)
print(f"Saved {len(crop_df)} crops with prompts to CSV and images to {save_dir}/")

crop_df

"""I then trained the CLIP model from scratch using SFT(supervised-fine tuning) to ensure it effectively generalizes to the specific characteristics of my dataset.









"""

from torch.utils.data import Dataset, DataLoader, random_split
from transformers import CLIPProcessor, CLIPModel
import torch
from torch.optim import AdamW
from PIL import Image, UnidentifiedImageError
from tqdm import tqdm
import pandas as pd

# ------------------- Dataset Class -------------------
class ClipFashionDatasetLocal(Dataset):
    def __init__(self, df):
        self.df = df.reset_index(drop=True)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        try:
            image = Image.open(row['image_path']).convert("RGB")
        except (UnidentifiedImageError, IOError):
            return None
        text = row["text"]
        return {"image": image, "text": text}

# ------------------- Collate Function -------------------
def collate_fn(batch):
    batch = [item for item in batch if item is not None]
    if not batch:
        return None
    images = [item["image"] for item in batch]
    texts = [item["text"] for item in batch]
    return processor(text=texts, images=images, return_tensors="pt", padding=True, truncation=True)

# ------------------- Setup -------------------
df = pd.read_csv("clip_crops_dataset_with_prompts.csv")

processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

dataset = ClipFashionDatasetLocal(df)

train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size
train_ds, val_ds = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_ds, batch_size=8, collate_fn=collate_fn)

# ------------------- Training Loop -------------------
optimizer = AdamW(model.parameters(), lr=5e-7)

def train_clip(model, train_loader, val_loader, processor, epochs=3):
    model.train()
    for epoch in range(epochs):
        print(f"\nEpoch {epoch + 1}/{epochs}")
        for batch in tqdm(train_loader):
            if batch is None:
                continue
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs, return_loss=True)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            print(f"Train Loss: {loss.item():.4f}")

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                if batch is None:
                    continue
                inputs = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**inputs, return_loss=True)
                val_loss += outputs.loss.item()
        val_loss /= len(val_loader)
        print(f"Validation Loss: {val_loss:.4f}")
        model.train()

    model.save_pretrained("finetuned_clip_fashion")
    processor.save_pretrained("finetuned_clip_fashion")
    torch.save(model.state_dict(), "finetuned_clip_fashion/clip_model.pth")
    print("Model saved to 'finetuned_clip_fashion/'")

# ------------------- Train -------------------
train_clip(model, train_loader, val_loader, processor, epochs=3)

"""Then stored the CLIP embeddings in the faiss database using the trained model"""

import pandas as pd
import torch
import numpy as np
import os
import json
from PIL import Image
from tqdm import tqdm
from transformers import CLIPProcessor, CLIPModel
import faiss

# ---------------- Setup ----------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model and processor
base_model_name = "openai/clip-vit-base-patch32"
clip_model = CLIPModel.from_pretrained(base_model_name).to(device)
clip_processor = CLIPProcessor.from_pretrained(base_model_name)

# Load fine-tuned weights
state_dict = torch.load("/content/finetuned_clip_fashion/clip_model.pth", map_location=device)
clip_model.load_state_dict(state_dict)
clip_model.eval()

# Load your DataFrame
# Example: crop_df = pd.read_csv("your_local_catalog.csv")
df_limited = crop_df  # You can also use .head(n) if needed

embeddings = []
metadata = []

print(f"Embedding {len(df_limited)} catalog images from local paths...")

for idx, row in tqdm(df_limited.iterrows(), total=len(df_limited)):
    product_id = str(row['id'])
    image_path = row['image_path']

    try:
        image = Image.open(image_path).convert("RGB")

        inputs = clip_processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            image_emb = clip_model.get_image_features(**inputs)
            image_emb = image_emb / image_emb.norm(p=2, dim=-1, keepdim=True)

        embeddings.append(image_emb.cpu().numpy())
        metadata.append({
            "faiss_index": len(embeddings) - 1,
            "product_id": product_id,
            "image_path": image_path
        })

    except Exception as e:
        print(f"❌ Failed [{product_id}] at {image_path}: {e}")

# Stack embeddings and build FAISS index
embedding_matrix = np.vstack(embeddings).astype('float32')
index = faiss.IndexFlatL2(embedding_matrix.shape[1])
index.add(embedding_matrix)

# Save index and metadata
os.makedirs("clip_faiss_test", exist_ok=True)
faiss.write_index(index, "clip_faiss_test/faiss_catalog.index")

with open("clip_faiss_test/metadata.json", "w") as f:
    json.dump(metadata, f, indent=2)

print("✅ Saved FAISS index and metadata from local image paths.")